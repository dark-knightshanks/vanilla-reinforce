# -*- coding: utf-8 -*-
"""vpg

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ooBtsf57RduNBza8RCUfbq6BzAmI0jYs
"""

!pip install gymnasium[classic-control] torch numpy matplotlib

import gymnasium as gym
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical
import numpy as np
from collections import deque
import matplotlib.pyplot as plt

# Policy Network
class PolicyNet(nn.Module):
    def __init__(self, obs_dim, hidden_dim, act_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = nn.Linear(obs_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, act_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        logits = self.fc2(x)
        return F.softmax(logits, dim=-1)

import random
seed = 96
torch.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)


#  Create environment
env = gym.make("CartPole-v1")

#  Instantiate policy and optimizer
policy = PolicyNet(env.observation_space.shape[0], 64, env.action_space.n)
optimizer = torch.optim.Adam(policy.parameters(), lr=0.01)

# Hyperparameters
gamma = 0.99
n_episode = 1
returns = deque(maxlen=100)
render_rate = 100  # render every N episodes

# Store average returns for plotting
avg_returns_list = []

while True:
    states, actions, rewards = [], [], []

    # Reset environment (new Gymnasium API)
    state, info = env.reset()
    done = False

    while not done:
        # Render occasionally
        if n_episode % render_rate == 0:
            env.render()

        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        probs = policy(state_tensor)
        sampler = Categorical(probs)
        action = sampler.sample()

        # Step in environment (new API)
        next_state, reward, terminated, truncated, _ = env.step(action.item())
        done = terminated or truncated

        # Store transition
        states.append(state)
        actions.append(action)
        rewards.append(reward)

        state = next_state

    # Compute discounted rewards-to-go
    discounted_returns = []
    G = 0
    for r in rewards[::-1]:
        G = r + gamma * G
        discounted_returns.insert(0, G)
    R = torch.tensor(discounted_returns, dtype=torch.float32)

    # Optional: normalize for stability
    #R = (R - R.mean()) / (R.std() + 1e-9)

    # Convert states and actions to tensors
    states_tensor = torch.tensor(states, dtype=torch.float32)
    actions_tensor = torch.tensor(actions)

    # Compute log probs
    probs = policy(states_tensor)
    sampler = Categorical(probs)
    log_probs = sampler.log_prob(actions_tensor)

    # Compute loss (gradient ascent)
    loss = -(log_probs * R).sum()

    # Update policy
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Log stats
    returns.append(sum(rewards))
    avg_return = np.mean(returns)
    avg_returns_list.append(avg_return)
    print(f"Episode: {n_episode:5d}\tAvg. Return: {avg_return:6.2f}")
    n_episode += 1


env.close()

# Plot learning curve
plt.plot(avg_returns_list)
plt.xlabel("Episodes")
plt.ylabel("Average Return")
plt.title("VPG on CartPole-v1")
plt.show()